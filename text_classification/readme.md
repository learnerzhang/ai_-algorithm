### 文本的表示模型
##### `词袋模型`
> 基本思想是把每篇文章看成一袋子词,并忽略每个词出现的顺序。
> 具体来看：将整段文本表示成一个长向量,每一维代表一个单词。该维对应的权重代表这个词在原文章中的重要程度。

##### `tf-idf`
- `TF−IDF(t,d)=TF(t,d)*IDF(t)`

- `TF(t,d)`为单词`t`在文档中出现的频率,`IDF(t)`是逆文档频率,用来衡量单词t对表达语义所起的重要性, 说明该词在文档中出现的频率越高,该词越不重要。
- 直观的解释是如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇
- 缺点: 单词程度的划分有时候并不是一个好的做法。比如Natural Language Processing一词，单个单词拆分的语义与三个词连续出现的语义并不相同。

- 改进: 通常可将连续出现的N个单词构成的词组作为一个特征放到向量表示中(`N-gram`)。


### 词嵌入与深度学习
##### word2vec
- word2vec实际上是一种浅层的神经网络模型,它有两种网络结构,分别是`CBOW`和`skip-gram`.
- `CBOW`: 目标是根据上下文出现的单词预测当前词的生成概率.

- `Skip-gram`: 根据当前词预测上下文各词的生成概率.

- `输入层`: 每个单词是由one-hot编码表示,所有词均表示一个N维向量,N为词汇表中单词的总数.在向量中,每个单词对应的维度为1，其余维度为0。

- `映射层`: K个隐含单元的值可以由N维输入向量以及连接输入和隐含单元的`N*K`维权重矩阵计算得到.

- `输出层`: 可以由隐含层向量(K维)，以及连接隐含层和输出层之间的`K*N`维权重矩阵计算得到.输出层也是一个N维向量,每一维与词汇表中的一个单词对应.最后对输出层向量应用`Softmax`函数,可以得到每个单词的生成概率.

- `训练神经网络权重`: 使得所有单词的整体生成概率最大化.共有两大参数:从输入层到隐含层的一个维度为`N*K`的权重矩阵,从隐含层到输出层的一个维度为`K*N`的权重矩阵。

- `寻优算法`: 学习权重可以使用`BP`算法实现

##### word2vec与LDA区别
- `LDA`是按照文档中单词的共现关系来对单词按照主题聚类,也可以理解为对“文档-单词”矩阵进行分解,得到`文档-主题`和`主题-单词`两个概率分布.
- `word2vec`实际上是对`上下文-单词`矩阵进行学习,其中上下文由周围几个单词组成,由此学到的词向量更多融入了上下文特征.

- 主题模型和词嵌入两类方法最大的不同在于模型本身:`主题模型`是一种基于概率图模型的生成式模型.其似然函数可以写为若干条件概率连乘的形式,其中包含需要推测的隐含变量(即主题);`词嵌入模型`一般表示为神经网络的形式,似然函数定义在网络的输出之上.需要学习网络的权重来得到单词的稠密向量表示.

##### RNN比CNN在文本处理的优缺点
- 传统文本处理任务的方法一般将`TF-IDF`向量作为特征输入,这样实际上`丢失`输入的文本系列中`每个单词的顺序`.
- `CNN`一般会接收一个定长的向量作为输入,然后通过`滑动窗口加池化`的方法将原来的输入转换为一个固定长度的向量表示.`优点`: 可以捕捉到文本中的一些局部特征,`缺点`: 两个单词之间的长距离依赖关系难以学习.
- `RNN`能够很好处理文本数据变长并且有序的输入序列.将前面阅读到的有用信息编码到状态变量中去,从而拥有了一定的记忆能力.

##### RNN梯度消失或梯度爆炸怎么解决

- `RNN`的求解可以采用`BPTT`(Back Propagation Through Time）算法实现(BP的简单变种).`RNN`设计的初衷在于捕捉长距离输入之间的依赖关系,然而使用BPTT的算法并不能成功捕捉远距离依赖关系,这一现象源于深度神经网络中的梯度消失问题.

- net^(t) = Ux^(t) + Wh^(t-1)
- h^(t) =  f(net^(t))
- y = g(Vh^(t))
- d(net^(t))/d(net^(t-1)) = d(net^(t))/d(h^(t-1))·d(h^(t-1))/d(net^(t-1)) = W·diag[f'(net^(t-1))]
- diag 雅克比矩阵
- 由于预测误差沿神经网络每一层反向传播,当雅克比矩阵最大特征值大于1时,随着离输出越来越远,每层的梯度大小会呈指数增长,导致梯度爆炸.反之若最大特征值小于1,梯度大小会指数减小,产生梯度消失.
- `梯度消失`意味着无法通过加深网络层数来提升预测效果,只有靠近输出的几层才真正起到学习的作用,这样RNN很难学习到输入序列中的长距离依赖关系.
- `梯度爆炸`可以通过`梯度裁剪`来缓解,即当梯度的范式大于某个给定值的时候,对梯度进行等比缩放.而`梯度消失`问题需要对模型本身进行改进.
- 深度残差网络是对前馈神经网络的改进,通过残差学习的方式缓解了梯度消失的现象,从而可以学习到更深层的网络表示.
- 对于`RNN`来说,`长短时记忆模型`及其变种门控循环单元等模型通过加入`门控机制`,很大程度上缓解了梯度消失带来的损失.


##### RNN中可以采用ReLU作为激活函数
- 可以采用`ReLU`,但是需要对`矩阵的初值做一定限制`,否则容易引发数值问题.
- net^t = Ux^t+Wh^{t−1}
- h^t = f(net^t)
- net^t = Ux^t+Wh^{t-1} = Ux^t + Wf(Ux^{t-1} + Wh^{t-2})
- 采用`ReLU`替换公式中的激活函数`f`,并且假设`ReLU`一直处于激活区域(即输入大于0),则有`f(x)=x`
- net^t = Ux^t+Wh^{t-1} = Ux^t + W(Ux^{t-1} + Wh^{t-2})
- 公式全部展开,`net`的表达式最终包含`t`个`W`连乘.如果`W`不是单位矩阵(对角线上为1,其余元素为0的矩阵),最终的结果将会趋于0或无穷,引发严重的数值问题.
- `CNN`中不会出现这样的问题,因为`CNN`中每一层的权重矩阵是不同的,并且在初始化的时候它们是`独立同分布`的,可以`相互抵消`,多层之后不会出现严重的数值问题.

- ∂net^t/∂net^{t-1} = W·diag[f'(net^(t-1))]
- 即使采用了`ReLU`激活函数,且一开始所有神经元都处于激活中(即输入大于0),diag[f'(net^(t-1))]为单位矩阵,∂net^t/∂net^{t-1}=W,经历了n层传递之后,有∂net^t/∂net^{t−1}=W^n. 即使使用`ReLU`,只要`W`不是单位矩阵,还是会出现`梯度消失`或`梯度爆炸`问题.

- 采用`ReLU`作为`RNN`中隐含层的激活函数时,只有当`W`的取值在单位矩阵附近时才能取得较好结果.
---

##### LSTM是如何实现长短期记忆功能


### 模型调优
- fine-tune
